# data_utils.py
import json
import os
import torch
from datasets import load_dataset
from config import CUTOFF_LEN, MODEL_CONFIG

def generate_training_data(data_point, tokenizer):
    """
    å°†è¾“å…¥å’Œè¾“å‡ºæ–‡æœ¬è½¬æ¢ä¸ºæ¨¡å‹å¯è¯»çš„tokensï¼Œæ”¯æŒå¤šç§æ¨¡å‹æ ¼å¼
    """
    try:
        # æ ¹æ®æ¨¡å‹ç±»å‹æ„å»ºæç¤ºè¯
        tokens = MODEL_CONFIG["special_tokens"]
        system_prompt = MODEL_CONFIG["system_prompt"]
        
        if MODEL_CONFIG["chat_template"] == "qwen":
            prompt = (
                f"{tokens['system_start']}{system_prompt}{tokens['system_end']}"
                f"{tokens['user_start']}{data_point['instruction']}\n{data_point['input']}{tokens['user_end']}"
                f"{tokens['assistant_start']}"
            )
            full_text = prompt + data_point["output"] + tokens["eos_token"]
        elif MODEL_CONFIG["chat_template"] == "llama":
            prompt = (
                f"{tokens['system_start']}{system_prompt}{tokens['system_end']}"
                f"{data_point['instruction']}\n{data_point['input']}{tokens['user_end']}"
                f"{tokens['assistant_start']}"
            )
            full_text = prompt + data_point["output"] + tokens["eos_token"]
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„èŠå¤©æ¨¡æ¿: {MODEL_CONFIG['chat_template']}")

        # è®¡ç®—ç”¨æˆ·æç¤ºè¯çš„tokenæ•°é‡
        prompt_tokenized = tokenizer(
            prompt,
            truncation=True,
            max_length=CUTOFF_LEN,
            padding=False,
            return_tensors=None
        )
        len_user_prompt_tokens = len(prompt_tokenized['input_ids'])

        # å°†å®Œæ•´çš„è¾“å…¥å’Œè¾“å‡ºè½¬æ¢ä¸ºtokens
        full_tokenized = tokenizer(
            full_text,
            truncation=True,
            max_length=CUTOFF_LEN,
            padding="max_length",
            return_tensors=None
        )

        input_ids = full_tokenized['input_ids']
        attention_mask = full_tokenized["attention_mask"]

        # åˆ›å»ºlabelsï¼Œå±è”½æç¤ºè¯éƒ¨åˆ†
        labels = input_ids.copy()
        for i in range(min(len_user_prompt_tokens, len(labels))):
            labels[i] = -100

        return {
            "input_ids": input_ids,
            "labels": labels,
            "attention_mask": attention_mask,
        }
    except Exception as e:
        print(f"âŒ æ•°æ®å¤„ç†é”™è¯¯: {e}")
        # è¿”å›é»˜è®¤çš„æ•°æ®
        return {
            "input_ids": [0] * CUTOFF_LEN,
            "labels": [-100] * CUTOFF_LEN,
            "attention_mask": [1] * CUTOFF_LEN,
        }

def load_and_preprocess_data(tokenizer, dataset_path, num_samples, val_size=0):
    """
    åŠ è½½å¹¶é¢„å¤„ç†è®­ç»ƒæ•°æ®
    """
    print(f"ğŸ“– åŠ è½½æ•°æ®é›†: {dataset_path}")
    print(f"ğŸ“Š æ•°æ®é‡: {num_samples}, éªŒè¯é›†æ¯”ä¾‹: {val_size}")
    
    try:
        # è¯»å–åŸå§‹æ•°æ®
        with open(dataset_path, "r", encoding="utf-8") as f:
            data_json = json.load(f)
        
        # é™åˆ¶æ•°æ®é‡
        selected_data = data_json[:num_samples]
        
        # åˆ›å»ºä¸´æ—¶æ•°æ®é›†æ–‡ä»¶
        tmp_dataset_path = "tmp_dataset.json"
        with open(tmp_dataset_path, "w", encoding="utf-8") as f:
            json.dump(selected_data, f, ensure_ascii=False, indent=2)
        
        # åŠ è½½æ•°æ®é›†
        dataset = load_dataset("json", data_files=tmp_dataset_path, download_mode="force_redownload")
        
        # é¢„å¤„ç†æ•°æ®
        if val_size > 0:
            # åˆ†å‰²è®­ç»ƒé›†å’ŒéªŒè¯é›†
            split = dataset["train"].train_test_split(test_size=val_size)
            train_dataset = split["train"].map(lambda x: generate_training_data(x, tokenizer))
            val_dataset = split["test"].map(lambda x: generate_training_data(x, tokenizer))
            
            print(f"âœ… æ•°æ®é¢„å¤„ç†å®Œæˆ - è®­ç»ƒé›†: {len(train_dataset)}, éªŒè¯é›†: {len(val_dataset)}")
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if os.path.exists(tmp_dataset_path):
                os.remove(tmp_dataset_path)
            
            return train_dataset, val_dataset
        else:
            # åªæœ‰è®­ç»ƒé›†
            train_dataset = dataset["train"].map(lambda x: generate_training_data(x, tokenizer))
            
            print(f"âœ… æ•°æ®é¢„å¤„ç†å®Œæˆ - è®­ç»ƒé›†: {len(train_dataset)}")
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if os.path.exists(tmp_dataset_path):
                os.remove(tmp_dataset_path)
            
            return train_dataset, None
            
    except Exception as e:
        print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if os.path.exists("tmp_dataset.json"):
            os.remove("tmp_dataset.json")
        raise

def create_data_collator():
    """åˆ›å»ºæ•°æ®æ”¶é›†å™¨"""
    def collate_fn(features):
        return {
            "input_ids": torch.tensor([f["input_ids"] for f in features], dtype=torch.long),
            "attention_mask": torch.tensor([f["attention_mask"] for f in features], dtype=torch.long),
            "labels": torch.tensor([f["labels"] for f in features], dtype=torch.long),
        }
    return collate_fn