# model_utils.py
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import LoraConfig, get_peft_model, PeftModel
from config import MODEL_PATH, TARGET_MODULES, LORA_CONFIG, DEVICE, MODEL_CONFIG

def load_model():
    """åŠ è½½åŸºç¡€æ¨¡å‹"""
    print(f"ğŸ“š åŠ è½½åŸºç¡€æ¨¡å‹: {MODEL_PATH}")
    try:
        model = AutoModelForCausalLM.from_pretrained(
            MODEL_PATH,
            torch_dtype=torch.float32,
            device_map=None,
            low_cpu_mem_usage=True
        )
        model = model.to(DEVICE)
        print(f"âœ… åŸºç¡€æ¨¡å‹åŠ è½½å®Œæˆï¼Œè®¾å¤‡: {DEVICE}")
        return model
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        raise

def load_tokenizer():
    """åŠ è½½åˆ†è¯å™¨"""
    print(f"ğŸ“š åŠ è½½åˆ†è¯å™¨: {MODEL_PATH}")
    try:
        tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
        tokenizer.pad_token = tokenizer.eos_token
        tokenizer.pad_token_id = tokenizer.eos_token_id if tokenizer.eos_token_id is not None else 0
        print("âœ… åˆ†è¯å™¨åŠ è½½å®Œæˆ")
        return tokenizer
    except Exception as e:
        print(f"âŒ åˆ†è¯å™¨åŠ è½½å¤±è´¥: {e}")
        raise

def apply_lora(model):
    """åº”ç”¨LoRAé…ç½®åˆ°æ¨¡å‹"""
    print("ğŸ”§ åº”ç”¨LoRAé…ç½®...")
    try:
        config = LoraConfig(target_modules=TARGET_MODULES, **LORA_CONFIG)
        peft_model = get_peft_model(model, config)
        print("âœ… LoRAé…ç½®åº”ç”¨å®Œæˆ")
        return peft_model
    except Exception as e:
        print(f"âŒ LoRAé…ç½®åº”ç”¨å¤±è´¥: {e}")
        raise

def load_finetuned_model(base_model, checkpoint_path):
    """åŠ è½½å¾®è°ƒåçš„æ¨¡å‹"""
    print(f"ğŸ”§ åŠ è½½å¾®è°ƒæƒé‡: {checkpoint_path}")
    try:
        model = PeftModel.from_pretrained(base_model, checkpoint_path, torch_dtype=torch.float32)
        model = model.to(DEVICE)
        print(f"âœ… å¾®è°ƒæ¨¡å‹åŠ è½½å®Œæˆï¼Œå‚æ•°ç±»å‹: {next(model.parameters()).dtype}")
        return model
    except Exception as e:
        print(f"âŒ å¾®è°ƒæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        raise

def build_prompt(instruction, input_text=""):
    """æ ¹æ®å½“å‰æ¨¡å‹é…ç½®æ„å»ºæç¤ºè¯"""
    tokens = MODEL_CONFIG["special_tokens"]
    system_prompt = MODEL_CONFIG["system_prompt"]
    
    if MODEL_CONFIG["chat_template"] == "qwen":
        prompt = (
            f"{tokens['system_start']}{system_prompt}{tokens['system_end']}"
            f"{tokens['user_start']}{instruction}\n{input_text}{tokens['user_end']}"
            f"{tokens['assistant_start']}"
        )
    elif MODEL_CONFIG["chat_template"] == "llama":
        prompt = (
            f"{tokens['system_start']}{system_prompt}{tokens['system_end']}"
            f"{instruction}\n{input_text}{tokens['user_end']}"
            f"{tokens['assistant_start']}"
        )
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„èŠå¤©æ¨¡æ¿: {MODEL_CONFIG['chat_template']}")
    
    return prompt

def clean_output(output):
    """æ¸…ç†æ¨¡å‹è¾“å‡º"""
    tokens = MODEL_CONFIG["special_tokens"]
    
    # ç§»é™¤assistantæ ‡è®°
    if tokens["assistant_start"] in output:
        output = output.split(tokens["assistant_start"])[-1]
    
    # ç§»é™¤ç»“æŸæ ‡è®°
    if tokens["assistant_end"] in output:
        output = output.split(tokens["assistant_end"])[0]
    
    return output.strip()