# KNN

KNN是基本分类与回归方法。

KNN的输入为实例的特征向量，对应于特征空间中的点；输出为实例的类别，可以取多类。

KNN假设给定一个训练数据集，其中实例类别已定。分类时，对新的实例，根据其k个最近邻的训练实例的类别，通过多数表决等方式进行预测。**不具有显式的学习过程**。

## K 近邻算法

给定一个数据集，对新的输入实例，在训练数据集中找到与该实例最邻近的k个实例，这k个实例的多数属于某个类，就把该输入实例分为这个类。

## K 近邻模型

k 近邻法使用的模型实际上对应于特征空间的划分。模型由三个基本要素——距离度量、k值的选择和分类决策规则决定的。

### 模型

当训练集、距离度量（如欧式距离）、k值及分类决策规则（多数表决）确定后，对于任何一个新的输入实例，它所属的类唯一地确定。相当于根据上述要素**将特征空间划分为一些子空间**，确定子空间里的每个点所属的类。

### 距离度量

特征空间中两个实例点的距离时两个实例点相似程度的反映。KNN 的特征空间一边是 n 维实数向量空间。使用的距离是 **欧氏距离** 。

### k 值的选择

k 值的选择会对 k 近邻法的结果产生重大影响。

在应用中，k值一般取一个比较小的数值。通常采用交叉验证法来选取最优的 k 值。

### 分类的决策

多数表决：如果分裂的损失函数为 0-1 损失函数，分类函数为：
$$
f:R^n \rarr \{c1,c2,...ck\}
$$
那么误分类的概率是：
$$
P(Y\neq f(X))=1 - P(Y=f(X))
$$
实例化，给定实例 x，其最邻近的 k 个训练实例点构成集合 N~k~(x)，如果N~k~(x)对应的类别是 c~j~，那么误分类率是：
$$
\frac{1}{k} \sum_{x_i∈N_k(x)}I(y_i \neq c_j) = 1 - \frac{1}{k}\sum_{x_i∈N_k(x)}I(y_i =c_j)
$$
要使误分类率最小，即经验风险最小化，就要使得等式右边 $\sum_{x_i∈N_k(x)}I(y_i =c_j)$ 最大。

## k 近邻法的实现：kd 树

主要考虑的问题是如何对训练数据进行快速 k 近邻搜索。这点在特征空间的维数大以及训练数据容量大时尤其必要。

最简单的实现方法是线性扫描，要计算输入实例与每一个训练实例的距离。当训练集很大时，计算非常耗时。

为了提高 k 近邻搜索的效率，可以考虑使用特征的结构存储训练数据，以减少计算距离的次数。

### 构造 kd 树

kd 树是二叉树，表示对k维空间的一个划分。

## 为什么树是 KNN 最佳搜索方法

1. 🌲 树结构通过 “空间划分” 快速排除不可能的点
   - 每个节点在构建时，都会将空间分为 **两半**
   - 当你查询时，首先去看你包含最近点的那半边
   - 只有当另一半“有可能更近”时，才会回溯搜索，减少了线性查找的工作量
2. 只访问 logN 级别的节点，而不是 N 个
3. 减枝机制可跳过大块无用区域

高维时，KD树也会退化为线性扫描。
