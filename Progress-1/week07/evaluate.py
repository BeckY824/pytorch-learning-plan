# evaluate.py
import torch
from transformers import GenerationConfig
from model_utils import build_prompt, clean_output

def evaluate(instruction, tokenizer, model, generation_config, max_len, input_text="", verbose=True):
    """
    ä½¿ç”¨å½“å‰æ¨¡å‹é…ç½®ç”Ÿæˆå“åº”ï¼Œæ”¯æŒå¤šç§æ¨¡å‹æ ¼å¼
    """
    # æ„å»ºæç¤ºè¯
    prompt = build_prompt(instruction, input_text)
    
    if verbose:
        print(f"ğŸ“ æç¤ºè¯: {prompt}")
    
    # ç¼–ç è¾“å…¥
    inputs = tokenizer(prompt, return_tensors='pt').to(model.device)
    input_ids = inputs["input_ids"]
    
    # ç”Ÿæˆå“åº”
    with torch.no_grad():
        generation_output = model.generate(
            input_ids=input_ids,
            generation_config=generation_config,
            max_new_tokens=max_len,
            return_dict_in_generate=True,
            output_scores=True,
            pad_token_id=tokenizer.pad_token_id
        )
    
    # è§£ç è¾“å‡º
    output = tokenizer.decode(generation_output.sequences[0], skip_special_tokens=False)
    
    # æ¸…ç†è¾“å‡º
    cleaned_output = clean_output(output)
    
    if verbose:
        print(f"ğŸ¯ ç”Ÿæˆç»“æœ: {cleaned_output}")
    
    return cleaned_output

def batch_evaluate(instructions, tokenizer, model, generation_config, max_len, input_texts=None, verbose=True):
    """
    æ‰¹é‡è¯„ä¼°å¤šä¸ªæŒ‡ä»¤
    """
    if input_texts is None:
        input_texts = [""] * len(instructions)
    
    if len(instructions) != len(input_texts):
        raise ValueError("æŒ‡ä»¤æ•°é‡å’Œè¾“å…¥æ–‡æœ¬æ•°é‡ä¸åŒ¹é…")
    
    results = []
    for i, (instruction, input_text) in enumerate(zip(instructions, input_texts)):
        if verbose:
            print(f"\nâ¡ï¸ å¤„ç†ç¬¬ {i+1}/{len(instructions)} ä¸ªæ ·ä¾‹")
        
        try:
            output = evaluate(instruction, tokenizer, model, generation_config, max_len, input_text, verbose)
            results.append({
                "instruction": instruction,
                "input": input_text,
                "output": output,
                "status": "success"
            })
        except Exception as e:
            error_msg = f"ç”Ÿæˆå¤±è´¥: {str(e)}"
            if verbose:
                print(f"âŒ {error_msg}")
            results.append({
                "instruction": instruction,
                "input": input_text,
                "output": "",
                "status": "error",
                "error": error_msg
            })
    
    return results

def print_results_sample(results, num_samples=3):
    """
    æ‰“å°ç»“æœæ ·ä¾‹
    """
    print(f"\nğŸ“Œ ç¤ºä¾‹ç»“æœå±•ç¤º (æ˜¾ç¤ºå‰ {num_samples} ä¸ª):")
    successful_results = [r for r in results if r.get("status") == "success"]
    
    if not successful_results:
        print("âŒ æ²¡æœ‰æˆåŠŸçš„ç”Ÿæˆç»“æœ")
        return
    
    for i, result in enumerate(successful_results[:num_samples]):
        print(f"\nç¬¬ {i+1} ä¸ªæ ·ä¾‹:")
        print(f"æŒ‡ä»¤: {result['instruction']}")
        if result['input']:
            print(f"è¾“å…¥: {result['input']}")
        print(f"ç”Ÿæˆ: {result['output']}")
        print("-" * 50)

def create_generation_config(temperature=0.1, top_p=0.3, no_repeat_ngram_size=3):
    """
    åˆ›å»ºç”Ÿæˆé…ç½®
    """
    return GenerationConfig(
        do_sample=True,
        temperature=temperature,
        num_beams=1,
        top_p=top_p,
        no_repeat_ngram_size=no_repeat_ngram_size
    )