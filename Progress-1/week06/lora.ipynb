{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "428fb505",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import json\n",
    "import warnings\n",
    "import logging\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import bitsandbytes as bnb\n",
    "from datasets import load_dataset, load_from_disk\n",
    "import transformers\n",
    "from peft import PeftModel\n",
    "from colorama import Fore, Style\n",
    "\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    GenerationConfig\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    prepare_model_for_kbit_training\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ee1925",
   "metadata": {},
   "source": [
    "## 定义辅助函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09aac226",
   "metadata": {},
   "source": [
    "在微调过程中，我们需要一些辅助函数来处理数据和评估模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d4a435",
   "metadata": {},
   "source": [
    "### 数据预处理函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "104d71c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_data(data_point):\n",
    "    \"\"\"\n",
    "    将输入和输出文本转换为模型可读取的 tokens。\n",
    "\n",
    "    参数：\n",
    "    - data_point: 包含 \"instruction\"、\"input\" 和 \"output\" 字段的字典。\n",
    "\n",
    "    返回：\n",
    "    - 包含模型输入 IDs、标签和注意力掩码的字典。\n",
    "    \n",
    "    示例:\n",
    "    - 如果你构建了一个字典 data_point_1，并包含字段 \"instruction\"、\"input\" 和 \"output\"，你可以像这样使用函数：\n",
    "        generate_training_data(data_point_1)\n",
    "    \"\"\"\n",
    "    # 构建完整的输入提示词\n",
    "    prompt = f\"\"\"\\\n",
    "[INST] <<SYS>>\n",
    "You are a helpful assistant and good at writing Tang poem. 你是一個樂於助人的助手且擅長寫唐詩。\n",
    "<</SYS>>\n",
    "\n",
    "{data_point[\"instruction\"]}\n",
    "{data_point[\"input\"]}\n",
    "[/INST]\"\"\"\n",
    "\n",
    "    # 计算用户提示词的 token 数量\n",
    "    len_user_prompt_tokens = (\n",
    "        len(\n",
    "            tokenizer(\n",
    "                prompt,\n",
    "                truncation=True,\n",
    "                max_length=CUTOFF_LEN + 1,\n",
    "                padding=\"max_length\",\n",
    "            )[\"input_ids\"]\n",
    "        ) - 1\n",
    "    )\n",
    "\n",
    "    # 将完整的输入和输出转换为 tokens\n",
    "    full_tokens = tokenizer(\n",
    "        prompt + \" \" + data_point[\"output\"] + \"</s>\",\n",
    "        truncation=True,\n",
    "        max_length=CUTOFF_LEN + 1,\n",
    "        padding=\"max_length\",\n",
    "    )[\"input_ids\"][:-1]\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": full_tokens,\n",
    "        \"labels\": [-100] * len_user_prompt_tokens + full_tokens[len_user_prompt_tokens:],\n",
    "        \"attention_mask\": [1] * len(full_tokens),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770c3241",
   "metadata": {},
   "source": [
    "### 模型评估函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d941e97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(instruction, generation_config, max_len, input_text=\"\", verbose=True):\n",
    "    \"\"\"\n",
    "    使用 Qwen 格式生成响应。\n",
    "    \"\"\"\n",
    "    prompt = (\n",
    "        \"<|im_start|>system\\n你是一位擅長寫唐詩的中文助手。\\n<|im_end|>\\n\"\n",
    "        f\"<|im_start|>user\\n{instruction}\\n{input_text}\\n<|im_end|>\\n\"\n",
    "        \"<|im_start|>assistant\\n\"\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        generation_output = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            generation_config=generation_config,\n",
    "            max_new_tokens=max_len,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True\n",
    "        )\n",
    "\n",
    "    output = tokenizer.decode(generation_output.sequences[0], skip_special_tokens=False)\n",
    "    # 清洗输出：截断 assistant 开头后面的内容\n",
    "    if \"<|im_start|>assistant\" in output:\n",
    "        output = output.split(\"<|im_start|>assistant\")[1]\n",
    "    if \"<|im_end|>\" in output:\n",
    "        output = output.split(\"<|im_end|>\")[0]\n",
    "    output = output.strip()\n",
    "\n",
    "    if verbose:\n",
    "        print(output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1e5e037b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 你可以（但不一定需要）更改 LLM 模型 \"\"\"\n",
    "\n",
    "model_name = \"Qwen/Qwen2-1.5B-Instruct\"\n",
    "\n",
    "# model_name = \"/content/TAIDE-LX-7B-Chat\"\n",
    "# 如果你想使用 TAIDE 模型，请先查看 TAIDE L Models Community License Agreement (https://drive.google.com/file/d/1FcUZjbUH6jr4xoCyAronN_slLgcdhEUd/view)。\n",
    "# 一旦使用，即表示你同意协议条款。\n",
    "# !wget -O taide_7b.zip \"https://www.dropbox.com/scl/fi/harnetdwx2ttq1xt94rin/TAIDE-LX-7B-Chat.zip?rlkey=yzyf5nxztw6farpwyyildx5s3&st=s22mz5ao&dl=0\"\n",
    "# !unzip taide_7b.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1bb67a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 使用設備：cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import logging\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    GenerationConfig\n",
    ")\n",
    "\n",
    "# ✅ 设置设备（MPS 优先）\n",
    "device = torch.device(\"cpu\")\n",
    "print(f\"✅ 使用設備：{device}\")\n",
    "\n",
    "# ✅ 本地模型路径（重点！直接指向模型文件夹）\n",
    "model_path = \"./cache/Qwen2-1.5B-Instruct\"  # ✅ 你的模型必须在这个路径下\n",
    "\n",
    "# ✅ 加载模型\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_path,  # ✅ 强调命名参数，避免误解为 repo id\n",
    "    low_cpu_mem_usage=True # MPS 仅支持 float32\n",
    ").to(device)\n",
    "\n",
    "# ✅ 加载 tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_path,\n",
    "    add_eos_token=True\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# ✅ 设置生成参数\n",
    "generation_config = GenerationConfig(\n",
    "    do_sample=True,\n",
    "    temperature=0.1,\n",
    "    num_beams=1,\n",
    "    top_p=0.3,\n",
    "    no_repeat_ngram_size=3,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b3837405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "print(model.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e849ad01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1:\n",
      "模型輸入:\n",
      "以下是一首唐詩的第一句話，請用你的知識判斷並完成整首詩。相見時難別亦難，東風無力百花殘。\n",
      "\n",
      "模型輸出:\n",
      "此詩為唐代詩人李商隱的《無题》詩，全詩如下：\n",
      "\n",
      "相見时难别亦难，东风无力百花残。\n",
      "\n",
      "春心莫共花争发，一寸相思一寸灰。\n",
      "\n",
      "此詩描寫了男女之間的難以割舍的情感，以及對別離的無奈和痛苦。首句「相見难」，暗示了男女兩人的相見之難，而「別亦难」則暗示了別離之難。第二句「東風无力百花殘」，描繪了春風无力，百花凋零的景象，\n",
      "--------------------------------------------------------------------------------\n",
      "Example 2:\n",
      "模型輸入:\n",
      "以下是一首唐詩的第一句話，請用你的知識判斷並完成整首詩。重帷深下莫愁堂，臥後清宵細細長。\n",
      "\n",
      "模型輸出:\n",
      "此詩為唐代詩人杜甫所作，原詩為：\n",
      "\n",
      "重帷深深下莫樓，臵酒清宵長細流。\n",
      "此詩描繪了一個女子在深夜時分，獨自坐在深處的樓上，與朋友共飲的情景。\n",
      "--------------------------------------------------------------------------------\n",
      "Example 3:\n",
      "模型輸入:\n",
      "以下是一首唐詩的第一句話，請用你的知識判斷並完成整首詩。芳辰追逸趣，禁苑信多奇。\n",
      "\n",
      "模型輸出:\n",
      "此詩為唐代詩人杜甫所作，整首诗如下：\n",
      "\n",
      "芳辰迎逸趣，\n",
      "禁苑见奇才。\n",
      "春色满园关不住，\n",
      "一枝红杏出墙来。\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 样例和 Prompt 都保持繁体 \"\"\"\n",
    "max_len = 128\n",
    "# 测试样例\n",
    "test_tang_list = [\n",
    "    '相見時難別亦難，東風無力百花殘。',\n",
    "    '重帷深下莫愁堂，臥後清宵細細長。',\n",
    "    '芳辰追逸趣，禁苑信多奇。'\n",
    "]\n",
    "\n",
    "# 获取每个样例的模型输出\n",
    "demo_before_finetune = []\n",
    "for tang in test_tang_list:\n",
    "    demo_before_finetune.append(\n",
    "        f'模型輸入:\\n以下是一首唐詩的第一句話，請用你的知識判斷並完成整首詩。{tang}\\n\\n模型輸出:\\n' +\n",
    "        evaluate('以下是一首唐詩的第一句話，請用你的知識判斷並完成整首詩。', generation_config, max_len, tang, verbose=False)\n",
    "    )\n",
    "\n",
    "# 打印并将输出存储到文本文件\n",
    "for idx in range(len(demo_before_finetune)):\n",
    "    print(f\"Example {idx + 1}:\")\n",
    "    print(demo_before_finetune[idx])\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25d3fb9",
   "metadata": {},
   "source": [
    "### 设置用于微调的超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c7b69c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用设备：cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "num_train_data = 1040  # 训练数据量，按需求调整\n",
    "\n",
    "output_dir = \"./output\"\n",
    "ckpt_dir = \"./exp1\"\n",
    "num_epoch = 1\n",
    "LEARNING_RATE = 3e-4\n",
    "\n",
    "cache_dir = \"./cache\"\n",
    "from_ckpt = False\n",
    "ckpt_name = None\n",
    "dataset_dir = \"./GenAI-Hw5/Tang_training_data.json\"\n",
    "\n",
    "logging_steps = 20\n",
    "save_steps = 65\n",
    "save_total_limit = 3\n",
    "report_to = \"none\"\n",
    "\n",
    "MICRO_BATCH_SIZE = 1  # 适配 MPS，显存有限，调小\n",
    "BATCH_SIZE = 4  # 如果可以，累积4个微批次\n",
    "GRADIENT_ACCUMULATION_STEPS = BATCH_SIZE // MICRO_BATCH_SIZE\n",
    "\n",
    "CUTOFF_LEN = 256\n",
    "\n",
    "LORA_R = 8\n",
    "LORA_ALPHA = 16\n",
    "LORA_DROPOUT = 0.05\n",
    "\n",
    "VAL_SET_SIZE = 0  # 不拆分验证集，节省计算\n",
    "\n",
    "# Qwen2-1.5B-Instruct 可能适用的 target_modules，保持简洁\n",
    "TARGET_MODULES = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
    "\n",
    "device_map = None  # MPS 设备不支持 device_map，加载后手动 to(device)\n",
    "\n",
    "world_size = 1\n",
    "ddp = False\n",
    "\n",
    "# 设备定义\n",
    "import torch\n",
    "device = torch.device(\"cpu\")\n",
    "print(f\"使用设备：{device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b7781c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型参数类型: torch.float32\n"
     ]
    }
   ],
   "source": [
    "print(\"模型参数类型:\", next(model.parameters()).dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0644bd19",
   "metadata": {},
   "source": [
    "### 开始微调"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4c92fdfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 1040 examples [00:00, 70167.07 examples/s]\n",
      "Map: 100%|██████████| 1040/1040 [00:00<00:00, 4343.62 examples/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "fp16 mixed precision requires a GPU (not 'mps').",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[64]\u001b[39m\u001b[32m, line 49\u001b[39m\n\u001b[32m     46\u001b[39m     val_data = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     48\u001b[39m \u001b[38;5;66;03m# 使用 Transformers Trainer 进行模型训练\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m trainer = \u001b[43mtransformers\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtransformers\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m        \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMICRO_BATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgradient_accumulation_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mGRADIENT_ACCUMULATION_STEPS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mLEARNING_RATE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogging_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogging_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m        \u001b[49m\u001b[43msave_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43msave_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mckpt_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[43m        \u001b[49m\u001b[43msave_total_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[43msave_total_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[43m        \u001b[49m\u001b[43mddp_find_unused_parameters\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mddp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 是否使用 DDP，控制梯度更新策略\u001b[39;49;00m\n\u001b[32m     63\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreport_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreport_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_collator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtransformers\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDataCollatorForLanguageModeling\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmlm\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[38;5;66;03m# 禁用模型的缓存功能\u001b[39;00m\n\u001b[32m     69\u001b[39m model.config.use_cache = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/image/lib/python3.11/site-packages/transformers/utils/deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/image/lib/python3.11/site-packages/transformers/trainer.py:471\u001b[39m, in \u001b[36mTrainer.__init__\u001b[39m\u001b[34m(self, model, args, data_collator, train_dataset, eval_dataset, processing_class, model_init, compute_loss_func, compute_metrics, callbacks, optimizers, optimizer_cls_and_kwargs, preprocess_logits_for_metrics)\u001b[39m\n\u001b[32m    469\u001b[39m \u001b[38;5;28mself\u001b[39m.is_in_train = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    470\u001b[39m \u001b[38;5;28mself\u001b[39m.model = model\n\u001b[32m--> \u001b[39m\u001b[32m471\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcreate_accelerator_and_postprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    473\u001b[39m \u001b[38;5;66;03m# memory metrics - must set up as early as possible\u001b[39;00m\n\u001b[32m    474\u001b[39m \u001b[38;5;28mself\u001b[39m._memory_tracker = TrainerMemoryTracker(\u001b[38;5;28mself\u001b[39m.args.skip_memory_metrics)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/image/lib/python3.11/site-packages/transformers/trainer.py:5176\u001b[39m, in \u001b[36mTrainer.create_accelerator_and_postprocess\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   5173\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mRequires accelerate>1.3.0 to use Tensor Parallelism.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   5175\u001b[39m \u001b[38;5;66;03m# create accelerator object\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m5176\u001b[39m \u001b[38;5;28mself\u001b[39m.accelerator = \u001b[43mAccelerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5177\u001b[39m \u001b[38;5;66;03m# some Trainer classes need to use `gather` instead of `gather_for_metrics`, thus we store a flag\u001b[39;00m\n\u001b[32m   5178\u001b[39m \u001b[38;5;28mself\u001b[39m.gather_function = \u001b[38;5;28mself\u001b[39m.accelerator.gather_for_metrics\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/image/lib/python3.11/site-packages/accelerate/accelerator.py:547\u001b[39m, in \u001b[36mAccelerator.__init__\u001b[39m\u001b[34m(self, device_placement, split_batches, mixed_precision, gradient_accumulation_steps, cpu, dataloader_config, deepspeed_plugin, fsdp_plugin, torch_tp_plugin, megatron_lm_plugin, rng_types, log_with, project_dir, project_config, gradient_accumulation_plugin, step_scheduler_with_optimizer, kwargs_handlers, dynamo_backend, dynamo_plugin, deepspeed_plugins)\u001b[39m\n\u001b[32m    536\u001b[39m \u001b[38;5;28mself\u001b[39m.native_amp = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    537\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.device.type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[32m    538\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mxpu\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    539\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    545\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33msdaa\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    546\u001b[39m ) \u001b[38;5;129;01mor\u001b[39;00m is_torch_xla_available(check_is_tpu=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m547\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mfp16 mixed precision requires a GPU (not \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.device.type\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m).\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    548\u001b[39m kwargs = \u001b[38;5;28mself\u001b[39m.scaler_handler.to_kwargs() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.scaler_handler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[32m    550\u001b[39m \u001b[38;5;66;03m# FSDP2 doesn't use ShardedGradScaler, don't want to modify `get_grad_scaler`, rather create a simple utility\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: fp16 mixed precision requires a GPU (not 'mps')."
     ]
    }
   ],
   "source": [
    "# 设置TOKENIZERS_PARALLELISM为false，这里简单禁用并行性以避免报错\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# 创建指定的输出目录\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(ckpt_dir, exist_ok=True)\n",
    "\n",
    "# 根据 from_ckpt 标志，从 checkpoint 加载模型权重\n",
    "if from_ckpt:\n",
    "    model = PeftModel.from_pretrained(model, ckpt_name)\n",
    "\n",
    "# 对量化模型进行预处理以进行训练\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# 使用 LoraConfig 配置 LORA 模型\n",
    "config = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    target_modules=TARGET_MODULES,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = get_peft_model(model, config)\n",
    "\n",
    "# 将 tokenizer 的填充 token 设置为 0\n",
    "tokenizer.pad_token_id = 0\n",
    "\n",
    "# 加载并处理训练数据\n",
    "with open(dataset_dir, \"r\", encoding=\"utf-8\") as f:\n",
    "    data_json = json.load(f)\n",
    "with open(\"tmp_dataset.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(data_json[:num_train_data], f, indent=2, ensure_ascii=False)\n",
    "\n",
    "data = load_dataset('json', data_files=\"tmp_dataset.json\", download_mode=\"force_redownload\")\n",
    "\n",
    "# 将训练数据分为训练集和验证集（若 VAL_SET_SIZE 大于 0）\n",
    "if VAL_SET_SIZE > 0:\n",
    "    train_val = data[\"train\"].train_test_split(\n",
    "        test_size=VAL_SET_SIZE, shuffle=True, seed=42\n",
    "    )\n",
    "    train_data = train_val[\"train\"].shuffle().map(generate_training_data)\n",
    "    val_data = train_val[\"test\"].shuffle().map(generate_training_data)\n",
    "else:\n",
    "    train_data = data['train'].shuffle().map(generate_training_data)\n",
    "    val_data = None\n",
    "\n",
    "# 使用 Transformers Trainer 进行模型训练\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=MICRO_BATCH_SIZE,\n",
    "        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "        num_train_epochs=num_epoch,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        logging_steps=logging_steps,\n",
    "        save_steps=save_steps,\n",
    "        output_dir=ckpt_dir,\n",
    "        save_total_limit=save_total_limit,\n",
    "        ddp_find_unused_parameters=False if ddp else None,  # 是否使用 DDP，控制梯度更新策略\n",
    "        report_to=report_to,\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "\n",
    "# 禁用模型的缓存功能\n",
    "model.config.use_cache = False\n",
    "\n",
    "# 若使用 PyTorch 2.0 以上版本且非 Windows 系统，编译模型\n",
    "if torch.__version__ >= \"2\" and sys.platform != 'win32':\n",
    "    model = torch.compile(model)\n",
    "\n",
    "# 开始模型训练\n",
    "trainer.train()\n",
    "\n",
    "# 将训练好的模型保存到指定目录\n",
    "model.save_pretrained(ckpt_dir)\n",
    "\n",
    "# 打印训练过程中可能出现的缺失权重警告信息\n",
    "print(\"\\n 如果上方有关于缺少键的警告，请忽略 :)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1944257c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "image",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
