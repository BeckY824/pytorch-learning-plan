# 感知机

二类分类的线性分类模型。

其输入为实例的特征向量，输出为实例的类别，取值 +1 和 -1。感知机对应输入空间（特征空间）中将实例划分为正负两类的超平面，属于判别模型。

是神经网络与支持向量机的基础。

## 感知机模型

$$
f(x) = sign(w*x+b)
$$

w，b 为感知机的模型参数，w 为权值 /权值向量，b 叫做偏置，$w*x$ 表示 w 和 x 的内积，sign是符号函数。

线性方程
$$
w * x + b = 0
$$
对应于特征空间 $R^n$ 中的一个超平面 $S$ , 其中 w 是超平面的法向量， b 是超平面的截距。这个超平面将特征空间划分为两个部分。位于两部分的点分别被分为正、负两类。因此，超平面 $S$ 称为分离超平面

![image-20250711095001237](/Users/edward_beck8n24/Library/Application Support/typora-user-images/image-20250711095001237.png)

## 感知机学习策略

### 数据集的线性可分性

如果存在某个超平面 S，能够将数据集的正实例点和负实例点完全正确地划分到超平面两侧，则称数据集 T 为线形可分数据集；否则，称数据集线性不可分。

### 感知机学习策略

假设训练数据集是线形可分，感知机的学习的目标是求得一个能将训练集正实例点和负实例点完全正确分开的分离超平面。为了找出这样的超平面，即确定感知机模型参数 w，b 需要确定一个学习策略，即定义损失函数并将损失函数极小化。

损失函数的一个自然选择是误分类点的总数。但是，这样的损失函数不是参数 w，b 的连续可导函数，不易优化。损失函数的另一个选择是误分类点到超平面 S 的总距离，这是感知机所采用的。

### 算法的收敛性

对于线性可分数据集，感知机学习算法原始形式收敛，即经过有限次迭代可以得到一个将训练集完全正确划分的分离超平面及感知机模型。

**Novikoff定理** ：因为数据线性可分，所以存在某个 w，使得所有样本被正确分类。**（之后进行证明）**

### 感知机学习算法的对偶形式

对偶形式的基本想法是：将 w 和 b 表示为实例 xi 和 yi 的线性组合形式，通过**求解其系数**而求得 w 和 b。
$$
w \larr w + \eta y_ix_i \newline
b \larr b + \eta y_i
$$
则 w，b 关于（xi，yi）的增量分别是 $a_iy_ix_i$  和 $a_iy_i$ ，这里 $a_i = n_i\eta$，$n_i$ 是点 (xi,yi) 被误分类的次数。这样最后学习到的 w，b 可以分别表示为：
$$
w = \sum_{i=1}^N a_iy_ix_i \newline
b = \sum_{i=1}^N a_iy_i
$$

## 对偶模式和传统梯度下降

| **项目**     | **原始形式（Primal）**             | **对偶形式（Dual）**              |
| ------------ | ---------------------------------- | --------------------------------- |
| **优化变量** | 直接优化模型参数（如 w, b）        | 优化拉格朗日乘子（如 $\alpha_i$） |
| **使用方法** | 梯度下降、SGD 等直接在参数空间优化 | 构造对偶问题再优化，常用于凸优化  |
| **常见场景** | 神经网络、线性回归等               | SVM、凸优化、拉格朗日乘子法       |

### 为什么要用对偶模式

对偶形式通常用于带约束的优化问题（尤其是凸优化），可以通过引入拉格朗日乘子将问题转换成只与这些乘子相关的优化问题。

| **维度**         | **原始形式（梯度下降）** | **对偶形式**                           |
| ---------------- | ------------------------ | -------------------------------------- |
| **优化变量**     | 模型参数本身（如 w）     | 拉格朗日乘子（如 $\alpha_i$）          |
| **目标函数维度** | 通常维度较低（如 d）     | 通常维度高（样本数 n）                 |
| **处理约束**     | 直接或用投影处理         | 对偶变量天然编码约束                   |
| **核方法支持**   | ❌ 很难处理非线性         | ✅ 通过核函数轻松引入非线性映射         |
| **可解释性**     | 模型参数直观易理解       | 对偶变量解释较难（但支持向量有解释性） |