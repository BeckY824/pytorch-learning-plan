# DPO微调示例：根据人类偏好优化LLM大语言模型

> 过去微调的方法通常依赖于问题和答案对，标注成本较高。2023年提出的 Direct Preference Optimization 提供了一种无需标注答案的高效微调方法。
>
> DPO 依赖于人类对文本的偏好对（preference pairs），数据集中只包含人类对两段文本中哪段更好的判断，而不是具体的正确答案。

完成DPO微调之后发现，1.5B的模型微调后的回答影响不是很大。虽然会根据我们偏好的比例进行回答，例如5:1，5个正向，1个反面。但是回答的结果差别不是很大。

很明显这和我们训练的模型大小与数据集有关，我们只用了50条数据。