# 模型参数与显存的关系

## 理论计算

神经网络模型由多个层组成，每一层都包含权重和偏置，这些统称为**模型参数**。模型的参数量一般直接影响它的学习能力和表示能力。

模型大小计算公式：
$$
模型大小(字节) = 参数数量✖️每个参数的字节数
$$
示例：

对于一个拥有10亿参数的模型，使用32位浮点数（float32），每个参数占用 32 ➗ 8 = 4 字节，即：
$$
模型大小 = 1000000000 ✖️ 4 = 4GB
$$
由此可得，十亿FB32参数预等于4GB显存。

亿以`meta-llama/Meta-Llama-3.1-70B-Instruct` 这个拥有700亿参数的大模型为例，仅考虑模型参数，它的显存就已经超过大多数消费级GPU（例如 RTX 4090 最高48G）：
$$
70 \times 10^9 \times 4 = 280GB
$$

## GPU显存需求

实际部署中，GPU不仅需要容纳模型参数，还要处理其他数据，这意味着更大的显存占用量。

**训练时的显存占用：**

- 模型参数
- 优化器状态：如动量和二阶矩估计等，取决于优化器的类型，单纯的SGD不占显存
- 梯度
- 中间激活值
- 批量大小
- 其他开销

**推理时的显存占用：**

- 模型参数
- 中间激活值
- 批量大小

**训练阶段**

以1B参数的模型为例，假设训练时使用 Adam，精度使用 FP32，仅考虑与模型参数挂钩的显存计算：

- 模型参数：4GB
- 梯度：4GB
- 优化器状态：8GB（2 $\times$ 4GB)

$$
总显存 = 4GB(参数) + 4GB(梯度) + 8GB（优化器） = 16GB
$$

## 不同精度的导入方式及其影响

为了降低显存占用，我们可以使用不同的数值精度格式来存储模型参数，这些精度格式在内存使用和计算性能上各有优劣。

### **常见的数值精度格式**

- FP 32，标准精度，每个参数占用 4 字节。
- FP 16，半精度浮点数，每个参数占用 2 字节。
- BF 16，与FP16类似，但具有更大的指数范围。
- INT 8，低精度整数，每个参数占用 1 字节。
- 量化格式，4位或更低，用于特殊的量化算法，进一步减少显存占用。

### 对显存占用的影响

使用更低的精度可以显著减少模型的显存占用：

- **FP16/BF16 相对于 FP32**：显存占用减半。
- **INT8 相对于 FP32**：显存占用减少到原来的四分之一。

**示例**：

对于一个 **1B** 参数的模型：

- **FP32 精度**：4 GB 显存。
- **FP16/BF16 精度**：2 GB 显存。
- **INT8 精度**：1 GB 显存。

**注意**：实际显存占用还受到其他因素影响，如 CUDA 上下文、中间激活值和显存碎片等，因此不会严格按照理论值减半或减少四分之一。对于较小的模型，差距可能不会那么显著。

## 精度的权衡与选择

### 准确性 vs. 性能

- **高精度（FP32）**：
  - **优点**：更高的数值稳定性和模型准确性。
  - **缺点**：占用**更多**显存，计算速度**较慢**。
- **低精度（FP16/INT8）**：
  - **优点**：占用**更少**的显存，计算速度**更快**。
  - **缺点**：可能**引入**数值误差，影响模型性能。

### 何时选择何种精度

- **FP32**：
  - 适用于训练**小型模型**或对数值精度要求**较高**的任务。
- **FP16/BF16**：
  - 适用于训练**大型模型**，利用混合精度（Mixed Precision）来节省显存并加速计算。
- **INT8**：
  - 主要用于**推理阶段**，尤其是在显存资源**有限**的情况下部署**超大模型**。

