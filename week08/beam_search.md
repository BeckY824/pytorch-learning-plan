# 深入理解 Beam Search：原理，示例与代码

属于**解码策略中的算法**，目标都是在 **序列生成任务中**，从**概率分布中选择最合理**的输出序列。

## 工作原理

宽度优先的算法，通过保留多个候选序列来探索可能的输出空间。

基本步骤：

1. 初始化：从一个初始序列（通常为空或特殊起始标记）开始，设定束宽 k，初始化候选序列集 B~0~ = start
2. 迭代生成：对于当前所有候选序列 B~t-1~ ，扩展一个新的词汇或符号，生成所有可能的下一个词汇组合，并计算每个序列的概率
3. 选择顶束：从所有扩展的候选中，选择得分最高的 k 个序列，作为下一个候选序列 B~t~
4. 终止条件：当所有候选序列都生成了结束标记(<eos>) ，或达到设定的最大长度 T，停止生成
5. 选择最终序列：从最终的序列集中，选择得分最高片的序列作为输出

## 如何处理 `<eos>`？

在每一步生成过程中，如果某个序列生成了 `<eos>`，则将其标记为完成，不再进行扩展。

- 假设在某一步，序列 ACB 扩展出 ACB`<eos>`，则：
  - ACB`<eos>`保留在最终候选集，并不再扩展
  - Beam Search 继续扩展其他未完成的序列，直到所有序列完成或达到最大长度

**问题：** 如果有一个序列被标记为完成，在下一个扩展步骤中，Beam Search 应该扩展多少个候选序列？

- 束宽（k）个，如果被标记完成的序列还保留在候选集中不移出，就是 k - 1 个

## 进一步深入 Beam Search

### 使用对数概率

实际应用中，尤其是在处理长序列时，直接相乘概率会导致数值下溢问题。通常使用对数概率来累加评分。

**示例：**

- 序列A 的概率为 0.4，其对数概率对 $log(0.4) = -0.916$
- 序列AC 的概率为 0.16，其对数概率为 $log(0.16)= -1.833$

在Beam Search 中，我们会选择对数概率较高（即绝对值较小）的序列作为顶束

### 参数

除了 `num_beams` 和 `early_stopping`，Beam Search 通常还涉及其他参数，以下是常见参数的简要解释：

- **`max_length`（最大生成长度）**：限制生成序列的最大长度。
- **`length_penalty`（长度惩罚）**：用于调整生成序列的长度偏好，通常用于平衡生成序列的长度与概率评分。值大于 1 时，会惩罚过长的序列，值小于 1 时，会鼓励生成较长的序列。
- **`no_repeat_ngram_size`**：防止生成序列中出现重复的 n-gram，提高生成内容的多样性。
- **`num_return_sequences`**：指定生成的序列数量，允许一次生成多个不同的候选序列，<= num_beams。

### 数学描述

**序列概率**

假设我们要生成一个长度为 T 的序列 Y = (y~1~,....,y~T~)，改序列的生成时逐步进行的，即每个词汇 y~t~ 的生成依赖于前面已经生成的词汇。因此，序列 Y 的联合概率为：
$$
P(Y) = \prod_{t=1}^T P(y_t|y_1,y_2,...,y_{t-1})
$$
**评分函数**

由于直接计算概率乘积在处理长序列时容易导致数值下溢，所以通过取对数来稳定数值并简化计算。取对数后的评分函数为：
$$
S(Y)=logP(Y)= \sum_{t=1}^TlogP(y_t|y_1,....,y_{t-1})
$$
模型的目标是最大化序列的概率：
$$
Y^* = arg max_YP(Y)
$$

## 其余算法以及应用

### Greedy Search

每一步都选择概率最高的词，不考虑整体最优

**实际应用**

- **BERT 预训练的 Mask 填充阶段**
  - 在 MLM（Masked Language Modeling）中进行预测时，常使用贪心策略快速填词。
- **部分语音识别模型的实时解码**
  - 如 DeepSpeech（早期版本）

### Top-k Sampling

每一步从概率最高的前 k 个词中随机采样一个作为输出

**实际应用**

- **GPT-2 / GPT-3 早期部署版本（如 OpenAI Playground）**