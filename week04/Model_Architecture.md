# 模型架构

## 大模型之模型概括

将语言模型（model）看作一个黑箱（black box），其可以根据输入需求的语言描述（prompt）生成符合需求的结果（completion），形式可以表达为：
$$
prompt \overset{model}{\leadsto} completion \ \ or \ \ model(prompt) = completion
$$
分词 & 模型架构：

- 即如何将一个字符串拆分成多个词元
- 真正实现大型语言模型的建模创新

## 分词

语言模型 p 是建立在词元（token）序列上的一个概率分布，其中每个词元来自某个词汇表，如下列形式：

```
[the, mouse, ate, the, cheese]
```

> 词元 token，通常指的是一个文本序列中的最小单元，可以是单词、数字、符号或其他类型的语言元素。

我们所日常了解的输入需要的是数值的，从而才能在模型中被计算，所以，如果输入是非数值类型的字符串是怎么处理的呢？

> 为什么说是“隐式的对齐”，这是由于每个词在模型中，都有一个其确定的词向量。例如 ～ 

### 基于空格的分词

最简单的方法是用 `text.split(' ')` 。仅仅通过空格来划分单词会带来很多问题。

目前从直觉和工程实践的角度来说：

- 首先我们不希望有太多的词元（极端情况：字符或字节），否则序列会变得难以建模
- 其次我们也不希望词元过少，否则单词之间就无法共享参数（例如， mother-in-law 和 father-in-law 应该完全不同吗？），这对于形态丰富的语言尤其是个问题。
- 每个词元应该是一个在语言或统计上有意义的单位。

### Byte pair encoding

将字节对编码算法应用于数据压缩领域，用于生成其中一个最常用的分词器。BPE分词器需要通过模型训练进行学习，获得需要分词文本的一些频率特征。

#### unicode的问题

unicode字符非常多。在训练数据中我们不可能见到所有的字符。为了进一步**减少数据的稀疏性**，我们可以对字节而不是unicode字符运行BPE算法。

### Unigram model

定义一个目标函数来捕捉一个好的分词的特征，这种基于目标函数的分词模型可以适应更好分词场景。

## 模型架构

### 语言模型分类

对于语言模型来说，最初的起源来自于Transformer模型，这个模型是编码-解码端 （Encoder-Decoder）的架构。但是当前对于语言模型的分类，将语言模型分为三个类型：编码端（Encoder-Only），解码端（Decoder-Only）和编码-解码端（Encoder-Decoder）。因此我们的架构展示以当前的分类展开。

# 新的模型架构

