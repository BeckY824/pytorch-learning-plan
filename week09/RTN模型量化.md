# 浅谈 RTN 模型量化: 非对称 vs 对称

> RTN（Round to Nearest）模型量化的背后究竟做了什么？后续的研究（GPTQ/AWQ/...）更像是对权重量化的范围做约束，本质上还是会使用类似的思想
>
> 以 INT8 为例，展示其中的一些原理

在量化后试着打印模型参数，发现量化主要是针对 **前馈层** 和 **注意力中的投影层** 进行。

简单来说这意味着对线性层进行量化，因为前馈层和投影层本质上都是**线性变化**。

## 非对称量化步骤

非对称量化的特点是量化范围在数轴的正负方向上不对称。具体步骤如下：

1. 确定范围

为了将 FP32 数值映射到 INT8，首先需要确定数值的最小值和最大值。这个范围通常通过分析模型某个部分或整个模型层在训练集上的数值分布来确定。

- 最小值：层输出的最小激活值
- 最大值：层输出的最大激活值

例如如果某层的 FP32 输出范围为 [-6.0, 6.0]

2. 计算缩放因子和零点

INT8 的取值范围为 [-128, 127]，总共有 256 个可能的值，因此定义：

- q~min~ : INT8 最小值，等于 -128
- q~max~: INT8 最大值，等于 127
- 缩放因子：

$$
scale = \frac{6-(-6)}{127-(-128)} = \frac{12}{255} = 0.047
$$

