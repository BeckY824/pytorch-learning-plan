{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "229331b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/image/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D(nf=2304, nx=768)\n",
      "          (c_proj): Conv1D(nf=768, nx=768)\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D(nf=3072, nx=768)\n",
      "          (c_proj): Conv1D(nf=768, nx=3072)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# 加载预训练的 GPT-2 模型和分词器\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "model = AutoModelForCausalLM.from_pretrained('gpt2')\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90781dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/image/lib/python3.11/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n",
      "/opt/anaconda3/envs/image/lib/python3.11/site-packages/peft/tuners/lora/layer.py:1803: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 使用 peft 库，轻松的将 LoRA 集成到模型中：\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "# 配置 LoRA\n",
    "lora_config = LoraConfig(\n",
    "    task_type = TaskType.CAUSAL_LM,\n",
    "    inference_mode = False,\n",
    "    r = 8,\n",
    "    lora_alpha = 32,\n",
    "    lora_dropout=0.1,\n",
    ")\n",
    "\n",
    "# 将 LoRA 应用到模型中\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97054cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): GPT2LMHeadModel(\n",
      "      (transformer): GPT2Model(\n",
      "        (wte): Embedding(50257, 768)\n",
      "        (wpe): Embedding(1024, 768)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "        (h): ModuleList(\n",
      "          (0-11): 12 x GPT2Block(\n",
      "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): GPT2Attention(\n",
      "              (c_attn): lora.Linear(\n",
      "                (base_layer): Conv1D(nf=2304, nx=768)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=768, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=2304, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (c_proj): Conv1D(nf=768, nx=768)\n",
      "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): GPT2MLP(\n",
      "              (c_fc): Conv1D(nf=3072, nx=768)\n",
      "              (c_proj): Conv1D(nf=768, nx=3072)\n",
      "              (act): NewGELUActivation()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 查看当前模型架构\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "900dc78e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 294,912 || all params: 124,734,720 || trainable%: 0.2364\n"
     ]
    }
   ],
   "source": [
    "# 查看增加的参数量\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a995e784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "可训练参数量: 294912\n",
      "总参数量: 124734720\n",
      "可训练参数占比: 0.24%\n"
     ]
    }
   ],
   "source": [
    "# 自定义函数查看参数\n",
    "def print_trainable_parameters(model):\n",
    "    trainable_params = 0\n",
    "    all_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        num_params = param.numel()\n",
    "        all_params += num_params\n",
    "        if param.requires_grad:\n",
    "            trainable_params += num_params\n",
    "    print(f\"可训练参数量: {trainable_params}\")\n",
    "    print(f\"总参数量: {all_params}\")\n",
    "    print(f\"可训练参数占比: {100 * trainable_params / all_params:.2f}%\")\n",
    "    \n",
    "print_trainable_parameters(model)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "888f0b66",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m      5\u001b[39m training_args = TrainingArguments(\n\u001b[32m      6\u001b[39m     output_dir=\u001b[33m'\u001b[39m\u001b[33m./results\u001b[39m\u001b[33m'\u001b[39m,         \u001b[38;5;66;03m# 模型保存和日志输出的目录路径\u001b[39;00m\n\u001b[32m      7\u001b[39m     num_train_epochs=\u001b[32m3\u001b[39m,             \u001b[38;5;66;03m# 训练的总轮数（epochs）\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     11\u001b[39m     save_steps=\u001b[32m100\u001b[39m,                 \u001b[38;5;66;03m# 每隔多少步保存模型\u001b[39;00m\n\u001b[32m     12\u001b[39m )\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# 创建 Trainer\u001b[39;00m\n\u001b[32m     15\u001b[39m trainer = Trainer(\n\u001b[32m     16\u001b[39m     model=model,                    \u001b[38;5;66;03m# 训练的模型对象，需要事先加载好\u001b[39;00m\n\u001b[32m     17\u001b[39m     args=training_args,             \u001b[38;5;66;03m# 上面定义的训练参数配置\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m     train_dataset=\u001b[43mtrain_dataset\u001b[49m,    \u001b[38;5;66;03m# 需要对应替换成已经处理过的dataset\u001b[39;00m\n\u001b[32m     19\u001b[39m )\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# 开始训练\u001b[39;00m\n\u001b[32m     22\u001b[39m trainer.train()\n",
      "\u001b[31mNameError\u001b[39m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# 准备数据集并进行微调\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# 定义训练参数\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',         # 模型保存和日志输出的目录路径\n",
    "    num_train_epochs=3,             # 训练的总轮数（epochs）\n",
    "    per_device_train_batch_size=16, # 每个设备（如GPU或CPU）上的训练批次大小，16表示每次输入模型的数据数量\n",
    "    learning_rate=5e-5,             # 学习率\n",
    "    logging_steps=10,               # 每隔多少步（steps）进行一次日志记录\n",
    "    save_steps=100,                 # 每隔多少步保存模型\n",
    ")\n",
    "\n",
    "# 创建 Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,                    # 训练的模型对象，需要事先加载好\n",
    "    args=training_args,             # 上面定义的训练参数配置\n",
    "    train_dataset=train_dataset,    # 需要对应替换成已经处理过的dataset\n",
    ")\n",
    "\n",
    "# 开始训练\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd759802",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1417036531.py, line 3)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mmodel.save_pretrained(./lora_model)\u001b[39m\n                          ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# 保存和加载 LoRA 微调的模型\n",
    "# 训练完成后，可以保存或者加载 LoRA 微调的参数\n",
    "model.save_pretrained(\"./lora_model\")\n",
    "\n",
    "# 在推理时，加载原始的预训练模型和 LoRA 参数\n",
    "# 加载原始模型\n",
    "base_model = AutoModelForCausalLM.from_pretrained('gpt2')\n",
    "\n",
    "# 加载 LoRA 参数\n",
    "from peft import PeftModel\n",
    "\n",
    "model = PeftModel.from_pretrained(base_model, './lora_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12226c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "image",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
