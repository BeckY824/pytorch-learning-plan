# 🧠 PyTorch + LLM 实践学习计划

本项目是一个系统性学习计划，旨在掌握训练大语言模型（LLM）、模型对齐、大规模训练、CUDA 编程等核心技术.

---

## 🎯 目标技能

- ✅ 熟练使用 PyTorch 进行模型训练
- ✅ 理解 Transformer 架构及其变体
- ✅ 熟悉 LLM 微调与多模态模型（如 CLIP、BLIP）
- ✅ 掌握模型对齐（RLHF、指令微调、偏差控制）
- ✅ 了解大规模分布式训练机制
- ✅ 理解并编写基础 CUDA kernel

---

## 🗓️ 学习计划概览（可持续更新）

| 周数 | 学习主题                                  | 内容关键词 |
|------|-------------------------------------------|-------------|
| Week 01 | PyTorch 基础 + 张量操作                   | `tensor`、`autograd`、`nn.Module` |
| Week 02 | 模型构建 + 训练流程                     | `Dataset`、`DataLoader`、`优化器` |
| Week 03 | Transformer + Attention 理解            | `Self-Attention`、`位置编码` |
| Week 04 | LLM 架构与训练机制                     | `GPT`, `BERT`, `tokenization` |
| Week 05 | 多模态模型：CLIP / BLIP 简介与实践     | `图文对齐`, `对比学习` |
| Week 06 | 模型对齐入门：指令微调 / RLHF 简介      | `alignment`, `preference modeling` |
| Week 07 | CUDA 编程初探 + 大规模训练介绍          | `GPU kernel`, `tensor core`, `DDP` |
| Week 08 | 实战项目：微调一个小 LLM 模型 + 总结    | `finetune`, `evaluation`, `report` |

---

## 学习进度 Week01

| 章节         | 状态 | 日期       | 自测 |    
|--------------|------|------------|-----------  
| 2.1 数据操作 | ✅   | 2025-05-22 | ✅ 2025-05-24   
| 2.2 数据预处理 |  ✅  | 2025-05-23  | ✅ 2025-05-26              
| 2.3 线性代数  | ✅  | 2025-05-24  | ✅ 2025-06-05   
| 2.4 微积分 |  ✅  |  2025-05-26 | ✅  2025-06-06  
| 2.5 自动微分 |  ✅ |  2025-06-05 |  ✅  2025-06-07      
| 2.6 概率   |   ✅  |  2025-06-06 |  ✅  2025-06-07    
| Pytorch基础自测  |  ✅ |   2025-06-07   |     

## 学习进度 Week02 (李沐动手学DL)

| 章节         | 状态 | 日期       | 自测 |    
|--------------|------|------------|-----------  
| 自注意力机制 | ✅   | 2025-06-09 ～ 06-12|    
| Transformer前馈网络，残差，编码器 |  ✅ | 2025-06-13 |  

## 学习进度 Week03（Happy LLM）

| 章节         | 状态 | 日期       | 内容 |
|--------------|------|------------|-----------  |
| 自注意力机制 | ✅   | 2025-06-15|  神经网络 & 注意力机制部分(单头)   |
|   -   |  ✅  |  2025-06-16 |    多头注意力 & Encoder，Decoder实现    |
|  | ✅ | 2025-06-17 | 手搓一个Transformer基础版 |
| 自测Transformer | ✅ | 2025-06-18 | 完成Transformer自测，结果一般，需加强。<br />**总体建议**：<br />-对attention原理有直接理解（缩放，softmax）<br />-能识别基本的结构<br />**需加强部分**：<br />-编码结构顺序&推理过程<br />-多头注意力拼接机制<br />-动手实现能力<br />-位置编码的数学表达于意义理解 |
| 注意力机制回顾 | ✅ | 2025-06-19 | 2 hours回顾，内容包括：<br />点积；简易attention实现；mask-self-attention；multi-head-attention实现与原理（利用矩阵的操作实现并行，先合后拆） |
| - | ✅ | 2025-06-20 | 回顾Encoder部分：FNN，LayerNorm，以及一个Encoder Layer组成 <br />对于多头部分的数学计算以及代码实现明天进行复习还有点部分没理解；<br />阅读了3.1部分的Encoder-only PLM |
|  | ✅ | 2025-06-21 | 回顾Decoder部分：Mask-attention，MultiHeadAttention，以及一个Decoder Layer组成 <br />阅读了3.2部分的Encoder-Decoder PLM以及3.3部分Decoder-only PLM |
| | ✅ | 2025-06-22 | 完成了一个完整的Transformer的源码手写，加深印象; <br />阅读4.1部分的什么是LLM |

## 学习进度 Week04

| 章节         | 状态 | 日期       | 内容                             |
| ------------ | ---- | ---------- | -------------------------------- |
| 自注意力机制 | ✅    | 2025-06-23 | 自注意力机制 & Transformer的自测 |
| 大模型基础   | ✅    | 2025-06-24 | 模型架构，  分词，语言模型分类   |

## 学习进度 Week05

| 章节           | 状态 | 日期       | 内容                        |
| -------------- | ---- | ---------- | --------------------------- |
| 大模型搭建实践 | ✅    | 2025-07-02 | 下载并阅读Qwen-3-0.6B大模型 |
| LoRA           | ✅    | 2025-07-03 | 完成LoRA基础知识的阅读      |
|                |      |            |                             |
