# 🧠 PyTorch + LLM 实践学习计划

本项目是一个系统性学习计划，旨在掌握训练大语言模型（LLM）、模型对齐、大规模训练、CUDA 编程等核心技术.

---

## 🎯 目标技能

- ✅ 熟练使用 PyTorch 进行模型训练
- ✅ 理解 Transformer 架构及其变体
- ✅ 熟悉 LLM 微调与多模态模型（如 CLIP、BLIP）
- ✅ 掌握模型对齐（RLHF、指令微调、偏差控制）
- ✅ 了解大规模分布式训练机制
- ✅ 理解并编写基础 CUDA kernel

---

## 🗓️ 学习计划概览（可持续更新）

| 周数 | 学习主题                                  | 内容关键词 |
|------|-------------------------------------------|-------------|
| Week 01 | PyTorch 基础 + 张量操作                   | `tensor`、`autograd`、`nn.Module` |
| Week 02 | 模型构建 + 训练流程                     | `Dataset`、`DataLoader`、`优化器` |
| Week 03 | Transformer + Attention 理解            | `Self-Attention`、`位置编码` |
| Week 04 | LLM 架构与训练机制                     | `GPT`, `BERT`, `tokenization` |
| Week 05 | 多模态模型：CLIP / BLIP 简介与实践     | `图文对齐`, `对比学习` |
| Week 06 | 模型对齐入门：指令微调 / RLHF 简介      | `alignment`, `preference modeling` |
| Week 07 | CUDA 编程初探 + 大规模训练介绍          | `GPU kernel`, `tensor core`, `DDP` |
| Week 08 | 实战项目：微调一个小 LLM 模型 + 总结    | `finetune`, `evaluation`, `report` |

---

## 学习进度 Week01

| 章节         | 状态 | 日期       | 自测 |    
|--------------|------|------------|-----------  
| 2.1 数据操作 | ✅   | 2025-05-22 | ✅ 2025-05-24   
| 2.2 数据预处理 |  ✅  | 2025-05-23  | ✅ 2025-05-26              
| 2.3 线性代数  | ✅  | 2025-05-24  | ✅ 2025-06-05   
| 2.4 微积分 |  ✅  |  2025-05-26 | ✅  2025-06-06  
| 2.5 自动微分 |  ✅ |  2025-06-05 |  ✅  2025-06-07      
| 2.6 概率   |   ✅  |  2025-06-06 |  ✅  2025-06-07    
| Pytorch基础自测  |  ✅ |   2025-06-07   |     

## 学习进度 Week02 (李沐动手学DL)

| 章节         | 状态 | 日期       | 自测 |    
|--------------|------|------------|-----------  
| 自注意力机制 | ✅   | 2025-06-09 ～ 06-12|    
| Transformer前馈网络，残差，编码器 |  ✅ | 2025-06-13 |  

## 学习进度 Week03（Happy LLM）

| 章节         | 状态 | 日期       | 内容 |
|--------------|------|------------|-----------  |
| 自注意力机制 | ✅   | 2025-06-15|  神经网络 & 注意力机制部分(单头)   |
|   -   |  ✅  |  2025-06-16 |    多头注意力 & Encoder，Decoder实现    |
|  | ✅ | 2025-06-17 | 手搓一个Transformer基础版 |
| 自测Transformer | ✅ | 2025-06-18 | 完成Transformer自测，结果一般，需加强。<br />**总体建议**：<br />-对attention原理有直接理解（缩放，softmax）<br />-能识别基本的结构<br />**需加强部分**：<br />-编码结构顺序&推理过程<br />-多头注意力拼接机制<br />-动手实现能力<br />-位置编码的数学表达于意义理解 |
| 注意力机制回顾 | ✅ | 2025-06-19 | 2 hours回顾，内容包括：<br />点积；简易attention实现；mask-self-attention；multi-head-attention实现与原理（利用矩阵的操作实现并行，先大后拆） |

 



