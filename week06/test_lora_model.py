#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
MacBook Air M4 LoRAæ¨¡å‹æµ‹è¯•è„šæœ¬
æµ‹è¯•è®­ç»ƒå®Œæˆçš„å”è¯—ç”Ÿæˆæ¨¡å‹
"""

# âœ… MPSè®¾å¤‡ç¯å¢ƒå˜é‡é…ç½®
import os
os.environ["ACCELERATE_MIXED_PRECISION"] = "no"
os.environ["ACCELERATE_USE_FP16"] = "false" 
os.environ["ACCELERATE_USE_BF16"] = "false"
os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ["PYTORCH_ENABLE_MPS_FALLBACK"] = "1"

import sys
import json
import warnings
import logging
warnings.filterwarnings("ignore")

import torch
import torch.nn as nn
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    GenerationConfig
)
from peft import PeftModel

def evaluate(instruction, generation_config, max_len, input_text="", verbose=True):
    """
    ä½¿ç”¨ Qwen æ ¼å¼ç”Ÿæˆå“åº”ã€‚
    """
    prompt = (
        "<|im_start|>system\nä½ æ˜¯ä¸€ä½æ“…é•·å¯«å”è©©çš„ä¸­æ–‡åŠ©æ‰‹ã€‚\n<|im_end|>\n"
        f"<|im_start|>user\n{instruction}\n{input_text}\n<|im_end|>\n"
        "<|im_start|>assistant\n"
    )

    inputs = tokenizer(prompt, return_tensors="pt").to(device)
    input_ids = inputs["input_ids"]

    with torch.no_grad():
        generation_output = model.generate(
            input_ids=input_ids,
            generation_config=generation_config,
            max_new_tokens=max_len,
            return_dict_in_generate=True,
            output_scores=True
        )

    output = tokenizer.decode(generation_output.sequences[0], skip_special_tokens=False)
    # æ¸…æ´—è¾“å‡ºï¼šæˆªæ–­ assistant å¼€å¤´åé¢çš„å†…å®¹
    if "<|im_start|>assistant" in output:
        output = output.split("<|im_start|>assistant")[1]
    if "<|im_end|>" in output:
        output = output.split("<|im_end|>")[0]
    output = output.strip()

    if verbose:
        print(output)
    return output

def main():
    global tokenizer, model, device
    
    print("ğŸš€ å¼€å§‹LoRAæ¨¡å‹æµ‹è¯•")
    print("âœ… MPSè®¾å¤‡ç¯å¢ƒå˜é‡å·²é…ç½®")
    
    # âœ… MacBook Air M4 è®¾å¤‡é…ç½®
    device = torch.device("mps" if torch.backends.mps.is_available() else "cpu")
    print(f"âœ… ä½¿ç”¨è®¾å¤‡ï¼š{device}")
    
    # é…ç½®å‚æ•°
    model_name = "Qwen/Qwen2-1.5B-Instruct"
    model_path = "./cache/Qwen2-1.5B-Instruct"  # æœ¬åœ°æ¨¡å‹è·¯å¾„
    ckpt_name = "./exp1"  # è®­ç»ƒå¥½çš„LoRAæƒé‡è·¯å¾„
    test_data_path = "GenAI-Hw5/Tang_testing_data.json"
    output_dir = "./output"
    output_path = os.path.join(output_dir, "results.txt")
    
    # ç”Ÿæˆå‚æ•°
    max_len = 128
    temperature = 0.1
    top_p = 0.3
    no_repeat_ngram_size = 3
    seed = 42
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(output_dir, exist_ok=True)
    
    print("ğŸ“š æ­£åœ¨åŠ è½½åŸºç¡€æ¨¡å‹...")
    
    # âœ… åŠ è½½ tokenizer - é€‚é…MacBook Air M4
    tokenizer = AutoTokenizer.from_pretrained(
        model_path,
        add_eos_token=True
    )
    tokenizer.pad_token = tokenizer.eos_token
    
    # âœ… åŠ è½½åŸºç¡€æ¨¡å‹ - ä½¿ç”¨float32ï¼Œä¸ä½¿ç”¨é‡åŒ–
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        torch_dtype=torch.float32,  # MPSä½¿ç”¨float32
        low_cpu_mem_usage=True,
        device_map=None  # MPSä¸æ”¯æŒauto device_map
    )
    
    print("ğŸ”§ æ­£åœ¨åŠ è½½LoRAæƒé‡...")
    
    # âœ… åŠ è½½å¾®è°ƒåçš„LoRAæƒé‡
    model = PeftModel.from_pretrained(
        model, 
        ckpt_name,
        torch_dtype=torch.float32
    )
    
    # å°†æ¨¡å‹ç§»åŠ¨åˆ°MPSè®¾å¤‡
    model.to(device)
    
    print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼Œå‚æ•°ç±»å‹: {next(model.parameters()).dtype}")
    
    # è®¾ç½®ç”Ÿæˆé…ç½®
    generation_config = GenerationConfig(
        do_sample=True,
        temperature=temperature,
        num_beams=1,
        top_p=top_p,
        no_repeat_ngram_size=no_repeat_ngram_size,
        pad_token_id=tokenizer.pad_token_id
    )
    
    # è¯»å–æµ‹è¯•æ•°æ®é›†
    print("ğŸ“– æ­£åœ¨è¯»å–æµ‹è¯•æ•°æ®...")
    with open(test_data_path, "r", encoding="utf-8") as f:
        test_datas = json.load(f)
    
    print(f"âœ… æ‰¾åˆ° {len(test_datas)} ä¸ªæµ‹è¯•æ ·ä¾‹")
    
    # å¼€å§‹æµ‹è¯•ç”Ÿæˆ
    print("ğŸ­ å¼€å§‹ç”Ÿæˆå”è¯—...")
    results = []
    
    # å¯¹æ¯ä¸ªæµ‹è¯•æ ·ä¾‹ç”Ÿæˆé¢„æµ‹ï¼Œå¹¶ä¿å­˜ç»“æœ
    with open(output_path, "w", encoding="utf-8") as f:
        for (i, test_data) in enumerate(test_datas):
            print(f"\nå¤„ç†ç¬¬ {i+1}/{len(test_datas)} ä¸ªæ ·ä¾‹...")
            print(f"è¾“å…¥: {test_data['input']}")
            
            try:
                predict = evaluate(
                    test_data["instruction"], 
                    generation_config, 
                    max_len, 
                    test_data["input"], 
                    verbose=False
                )
                
                result_line = f"{i+1}. " + test_data["input"] + predict
                f.write(result_line + "\n")
                
                print(f"ç”Ÿæˆ: {predict}")
                print(f"å®Œæ•´ç»“æœ: {result_line}")
                
                results.append({
                    "input": test_data["input"],
                    "output": predict,
                    "full_poem": result_line
                })
                
            except Exception as e:
                error_msg = f"ç¬¬ {i+1} ä¸ªæ ·ä¾‹ç”Ÿæˆå¤±è´¥: {e}"
                print(f"âŒ {error_msg}")
                f.write(f"{i+1}. {test_data['input']} [ç”Ÿæˆå¤±è´¥: {e}]\n")
    
    print(f"\nğŸ‰ æµ‹è¯•å®Œæˆï¼")
    print(f"ğŸ“ ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
    print(f"âœ… æˆåŠŸç”Ÿæˆ {len([r for r in results if 'output' in r])} é¦–å”è¯—")
    
    # æ˜¾ç¤ºå‡ ä¸ªç¤ºä¾‹ç»“æœ
    print("\nğŸ“ éƒ¨åˆ†ç”Ÿæˆç¤ºä¾‹:")
    for i, result in enumerate(results[:3]):  # æ˜¾ç¤ºå‰3ä¸ªç»“æœ
        print(f"\nç¤ºä¾‹ {i+1}:")
        print(f"è¾“å…¥: {result['input']}")
        print(f"ç”Ÿæˆ: {result['output']}")
        print("-" * 50)
    
    return True

if __name__ == "__main__":
    try:
        success = main()
        if success:
            print("\nğŸŠ LoRAæ¨¡å‹æµ‹è¯•æˆåŠŸå®Œæˆï¼")
        else:
            print("\nğŸ’¡ æµ‹è¯•é‡åˆ°é—®é¢˜ï¼Œè¯·æ£€æŸ¥æ—¥å¿—")
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc() 