#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
MacBook Air M4 LoRAè®­ç»ƒè„šæœ¬ - ä½¿ç”¨ç”¨æˆ·æŒ‡å®šçš„å‚æ•°é…ç½®
è§£å†³MPSè®¾å¤‡å…¼å®¹æ€§é—®é¢˜
"""

# âœ… å…³é”®ä¿®å¤ï¼šåœ¨å¯¼å…¥transformersä¹‹å‰è®¾ç½®ç¯å¢ƒå˜é‡
import os
os.environ["ACCELERATE_MIXED_PRECISION"] = "no"
os.environ["ACCELERATE_USE_FP16"] = "false" 
os.environ["ACCELERATE_USE_BF16"] = "false"
os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ["PYTORCH_ENABLE_MPS_FALLBACK"] = "1"

import sys
import json
import warnings
import logging
warnings.filterwarnings("ignore")

import torch
import torch.nn as nn
import bitsandbytes as bnb
from datasets import load_dataset, load_from_disk
import transformers
from peft import PeftModel
from colorama import Fore, Style

from tqdm import tqdm
from transformers import (
    AutoTokenizer,
    AutoConfig,
    AutoModelForCausalLM,
    BitsAndBytesConfig,
    GenerationConfig
)
from peft import (
    LoraConfig,
    get_peft_model,
    get_peft_model_state_dict,
    prepare_model_for_kbit_training
)

def generate_training_data(data_point):
    """
    å°†è¾“å…¥å’Œè¾“å‡ºæ–‡æœ¬è½¬æ¢ä¸ºæ¨¡å‹å¯è¯»å–çš„ tokensã€‚
    """
    try:
        # æ„å»ºå®Œæ•´çš„è¾“å…¥æç¤ºè¯
        prompt = f"""[INST] <<SYS>>
You are a helpful assistant and good at writing Tang poem. ä½ æ˜¯ä¸€å€‹æ¨‚æ–¼åŠ©äººçš„åŠ©æ‰‹ä¸”æ“…é•·å¯«å”è©©ã€‚
<</SYS>>

{data_point["instruction"]}
{data_point["input"]}
[/INST]"""

        # è®¡ç®—ç”¨æˆ·æç¤ºè¯çš„ token æ•°é‡
        prompt_tokenized = tokenizer(
            prompt,
            truncation=True,
            max_length=CUTOFF_LEN,
            padding=False,
            return_tensors=None
        )
        len_user_prompt_tokens = len(prompt_tokenized["input_ids"])

        # å°†å®Œæ•´çš„è¾“å…¥å’Œè¾“å‡ºè½¬æ¢ä¸º tokens
        full_text = prompt + " " + data_point["output"] + "</s>"
        full_tokenized = tokenizer(
            full_text,
            truncation=True,
            max_length=CUTOFF_LEN,
            padding="max_length",
            return_tensors=None
        )
        
        input_ids = full_tokenized["input_ids"]
        attention_mask = full_tokenized["attention_mask"]
        
        # åˆ›å»ºlabelsï¼Œå±è”½æç¤ºè¯éƒ¨åˆ†
        labels = input_ids.copy()
        for i in range(min(len_user_prompt_tokens, len(labels))):
            labels[i] = -100

        return {
            "input_ids": input_ids,
            "labels": labels,
            "attention_mask": attention_mask,
        }
    except Exception as e:
        print(f"æ•°æ®å¤„ç†é”™è¯¯: {e}")
        # è¿”å›é»˜è®¤çš„æ•°æ®
        return {
            "input_ids": [0] * CUTOFF_LEN,
            "labels": [-100] * CUTOFF_LEN,
            "attention_mask": [1] * CUTOFF_LEN,
        }

def evaluate(instruction, generation_config, max_len, input_text="", verbose=True):
    """
    ä½¿ç”¨ Qwen æ ¼å¼ç”Ÿæˆå“åº”ã€‚
    """
    prompt = (
        "<|im_start|>system\nä½ æ˜¯ä¸€ä½æ“…é•·å¯«å”è©©çš„ä¸­æ–‡åŠ©æ‰‹ã€‚\n<|im_end|>\n"
        f"<|im_start|>user\n{instruction}\n{input_text}\n<|im_end|>\n"
        "<|im_start|>assistant\n"
    )

    inputs = tokenizer(prompt, return_tensors="pt").to(device)
    input_ids = inputs["input_ids"]

    with torch.no_grad():
        generation_output = model.generate(
            input_ids=input_ids,
            generation_config=generation_config,
            max_new_tokens=max_len,
            return_dict_in_generate=True,
            output_scores=True
        )

    output = tokenizer.decode(generation_output.sequences[0], skip_special_tokens=False)
    # æ¸…æ´—è¾“å‡ºï¼šæˆªæ–­ assistant å¼€å¤´åé¢çš„å†…å®¹
    if "<|im_start|>assistant" in output:
        output = output.split("<|im_start|>assistant")[1]
    if "<|im_end|>" in output:
        output = output.split("<|im_end|>")[0]
    output = output.strip()

    if verbose:
        print(output)
    return output

def main():
    global tokenizer, model, device, CUTOFF_LEN
    
    print("ğŸš€ å¼€å§‹MacBook Air M4 LoRAè®­ç»ƒ")
    print("âœ… MPSè®¾å¤‡ç¯å¢ƒå˜é‡å·²é…ç½®")
    
    # âœ… MacBook Air M4 è®¾å¤‡é…ç½®
    device = torch.device("mps" if torch.backends.mps.is_available() else "cpu")
    print(f"âœ… ä½¿ç”¨è¨­å‚™ï¼š{device}")
    
    """ å¼ºçƒˆå»ºè®®ä½ å°è¯•è°ƒæ•´è¿™ä¸ªå‚æ•° """
    num_train_data = 520  # è®¾ç½®ç”¨äºè®­ç»ƒçš„æ•°æ®é‡
    
    """ ä½ å¯ä»¥ï¼ˆä½†ä¸ä¸€å®šéœ€è¦ï¼‰æ›´æ”¹è¿™äº›è¶…å‚æ•° """
    output_dir = "./output"
    ckpt_dir = "./exp1"
    num_epoch = 1
    LEARNING_RATE = 3e-4
    
    """ å»ºè®®ä¸è¦æ›´æ”¹æ­¤å•å…ƒæ ¼ä¸­çš„ä»£ç  """
    cache_dir = "./cache"
    from_ckpt = False
    ckpt_name = None
    dataset_dir = "./GenAI-Hw5/Tang_training_data.json"
    logging_steps = 20
    save_steps = 65
    save_total_limit = 3
    report_to = "none"
    
    # âœ… é’ˆå¯¹MPSè®¾å¤‡è°ƒæ•´çš„æ‰¹æ¬¡å¤§å°
    MICRO_BATCH_SIZE = 2  # é™ä½ä»¥é€‚é…MPSæ˜¾å­˜
    BATCH_SIZE = 8        # é™ä½ä»¥é€‚é…MPSæ˜¾å­˜
    GRADIENT_ACCUMULATION_STEPS = BATCH_SIZE // MICRO_BATCH_SIZE
    CUTOFF_LEN = 256
    LORA_R = 8
    LORA_ALPHA = 16
    LORA_DROPOUT = 0.05
    VAL_SET_SIZE = 0
    TARGET_MODULES = ["q_proj", "up_proj", "o_proj", "k_proj", "down_proj", "gate_proj", "v_proj"]
    
    # âœ… MPSè®¾å¤‡é…ç½® (ä¸ä½¿ç”¨auto device_map)
    device_map = None  # MPSä¸æ”¯æŒauto device_map
    world_size = int(os.environ.get("WORLD_SIZE", 1))
    ddp = world_size != 1
    if ddp:
        device_map = {"": int(os.environ.get("LOCAL_RANK") or 0)}
        GRADIENT_ACCUMULATION_STEPS = GRADIENT_ACCUMULATION_STEPS // world_size
    
    # åˆ›å»ºæŒ‡å®šçš„è¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)
    os.makedirs(ckpt_dir, exist_ok=True)
    
    # âœ… åŠ è½½æ¨¡å‹ - é€‚é…MPSè®¾å¤‡
    model_path = "./cache/Qwen2-1.5B-Instruct"
    
    print("ğŸ“š æ­£åœ¨åŠ è½½æ¨¡å‹...")
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        low_cpu_mem_usage=True,
        torch_dtype=torch.float32,  # MPSä½¿ç”¨float32
        device_map=device_map
    )
    
    tokenizer = AutoTokenizer.from_pretrained(
        model_path,
        add_eos_token=True
    )
    tokenizer.pad_token = tokenizer.eos_token
    
    print(f"âœ… æ¨¡å‹å‚æ•°ç±»å‹: {next(model.parameters()).dtype}")
    
    # æ ¹æ® from_ckpt æ ‡å¿—ï¼Œä» checkpoint åŠ è½½æ¨¡å‹æƒé‡
    if from_ckpt:
        model = PeftModel.from_pretrained(model, ckpt_name)
    
    # âœ… å¯¹äºMPSè®¾å¤‡ï¼Œè·³è¿‡é‡åŒ–é¢„å¤„ç†ï¼ˆå› ä¸ºæˆ‘ä»¬ä½¿ç”¨float32ï¼‰
    # model = prepare_model_for_kbit_training(model)  # æ³¨é‡Šæ‰ï¼Œå› ä¸ºæˆ‘ä»¬ä¸ä½¿ç”¨é‡åŒ–
    
    # ä½¿ç”¨ LoraConfig é…ç½® LORA æ¨¡å‹
    config = LoraConfig(
        r=LORA_R,
        lora_alpha=LORA_ALPHA,
        target_modules=TARGET_MODULES,
        lora_dropout=LORA_DROPOUT,
        bias="none",
        task_type="CAUSAL_LM",
    )
    model = get_peft_model(model, config)
    
    # âœ… å°†æ¨¡å‹ç§»åŠ¨åˆ°MPSè®¾å¤‡
    model.to(device)
    
    # å°† tokenizer çš„å¡«å…… token è®¾ç½®ä¸º 0
    tokenizer.pad_token_id = 0
    
    # åŠ è½½å¹¶å¤„ç†è®­ç»ƒæ•°æ®
    print(f"ğŸ“Š æ­£åœ¨åŠ è½½ {num_train_data} æ¡è®­ç»ƒæ•°æ®...")
    with open(dataset_dir, "r", encoding="utf-8") as f:
        data_json = json.load(f)
    with open("tmp_dataset.json", "w", encoding="utf-8") as f:
        json.dump(data_json[:num_train_data], f, indent=2, ensure_ascii=False)
    
    data = load_dataset('json', data_files="tmp_dataset.json", download_mode="force_redownload")
    
    # å°†è®­ç»ƒæ•°æ®åˆ†ä¸ºè®­ç»ƒé›†å’ŒéªŒè¯é›†ï¼ˆè‹¥ VAL_SET_SIZE å¤§äº 0ï¼‰
    if VAL_SET_SIZE > 0:
        train_val = data["train"].train_test_split(
            test_size=VAL_SET_SIZE, shuffle=True, seed=42
        )
        train_data = train_val["train"].shuffle().map(
            generate_training_data,
            remove_columns=train_val["train"].column_names
        )
        val_data = train_val["test"].shuffle().map(
            generate_training_data,
            remove_columns=train_val["test"].column_names
        )
    else:
        train_data = data['train'].shuffle().map(
            generate_training_data,
            remove_columns=data['train'].column_names
        )
        val_data = None
    
    print("ğŸ”„ æ•°æ®å¤„ç†å®Œæˆï¼Œå¼€å§‹è®­ç»ƒé…ç½®...")
    
    # âœ… ç®€åŒ–çš„DataCollator
    def data_collator(features):
        batch = {}
        batch["input_ids"] = torch.tensor([f["input_ids"] for f in features], dtype=torch.long)
        batch["attention_mask"] = torch.tensor([f["attention_mask"] for f in features], dtype=torch.long)
        batch["labels"] = torch.tensor([f["labels"] for f in features], dtype=torch.long)
        return batch
    
    # âœ… ä½¿ç”¨ Transformers Trainer è¿›è¡Œæ¨¡å‹è®­ç»ƒ - MPSå…¼å®¹é…ç½®
    trainer = transformers.Trainer(
        model=model,
        train_dataset=train_data,
        eval_dataset=val_data,
        args=transformers.TrainingArguments(
            per_device_train_batch_size=MICRO_BATCH_SIZE,
            gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,
            warmup_steps=50,
            num_train_epochs=num_epoch,
            learning_rate=LEARNING_RATE,
            fp16=False,  # âœ… ç¦ç”¨fp16ä»¥å…¼å®¹MPS
            bf16=False,  # âœ… ç¦ç”¨bf16ä»¥å…¼å®¹MPS
            dataloader_pin_memory=False,  # âœ… MPSå…¼å®¹æ€§
            logging_steps=logging_steps,
            save_strategy="steps",
            save_steps=save_steps,
            output_dir=ckpt_dir,
            save_total_limit=save_total_limit,
            ddp_find_unused_parameters=False if ddp else None,
            report_to=report_to,
            remove_unused_columns=False,  # âœ… ä¿æŒæ•°æ®å®Œæ•´æ€§
            max_grad_norm=1.0,  # âœ… æ¢¯åº¦è£å‰ª
        ),
        data_collator=data_collator,  # âœ… ä½¿ç”¨ç®€åŒ–çš„data_collator
    )
    
    # ç¦ç”¨æ¨¡å‹çš„ç¼“å­˜åŠŸèƒ½
    model.config.use_cache = False
    
    print("âœ… å¼€å§‹è®­ç»ƒ...")
    
    # âœ… å¼€å§‹æ¨¡å‹è®­ç»ƒ
    try:
        trainer.train()
        print("âœ… è®­ç»ƒå®Œæˆï¼")
        
        # å°†è®­ç»ƒå¥½çš„æ¨¡å‹ä¿å­˜åˆ°æŒ‡å®šç›®å½•
        model.save_pretrained(ckpt_dir)
        print(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ°: {ckpt_dir}")
        
        print("\nå¦‚æœä¸Šæ–¹æœ‰å…³äºç¼ºå°‘é”®çš„è­¦å‘Šï¼Œè¯·å¿½ç•¥ :)")
        return True
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒå‡ºé”™: {e}")
        return False

if __name__ == "__main__":
    success = main()
    if success:
        print("\nğŸ‰ LoRAå¾®è°ƒæˆåŠŸå®Œæˆï¼")
        print("ğŸ“ æ¨¡å‹æ–‡ä»¶å·²ä¿å­˜åˆ° ./exp1/ ç›®å½•")
        print("ğŸ“ Checkpoints å·²ä¿å­˜åˆ° ./exp1/checkpoint-* ç›®å½•")
    else:
        print("\n è®­ç»ƒé‡åˆ°é—®é¢˜ï¼Œè¯·æ£€æŸ¥æ—¥å¿—ä¿¡æ¯") 