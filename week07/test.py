# test.py
import os
import json
import torch
from transformers import GenerationConfig
from config import *
from model_utils import load_model, load_tokenizer, load_finetuned_model
from evaluate import evaluate, batch_evaluate, print_results_sample, create_generation_config

def test():
    """
    æµ‹è¯•å¾®è°ƒåçš„æ¨¡å‹
    """
    print("ğŸš€ å¼€å§‹ LoRA æ¨¡å‹æµ‹è¯•")
    print(f"ğŸ”§ å½“å‰æ¨¡å‹: {CURRENT_MODEL}")
    print(f"ğŸ“ æ¨¡å‹è·¯å¾„: {MODEL_PATH}")
    print(f"ğŸ’¾ æ£€æŸ¥ç‚¹ç›®å½•: {CKPT_DIR}")
    print(f"ğŸ“Š æµ‹è¯•æ•°æ®: {TEST_DATA_PATH}")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    output_path = os.path.join(OUTPUT_DIR, "test_results.txt")
    
    # åŠ è½½åˆ†è¯å™¨
    tokenizer = load_tokenizer()
    
    # åŠ è½½åŸºç¡€æ¨¡å‹
    base_model = load_model()
    
    # åŠ è½½å¾®è°ƒæƒé‡
    model = load_finetuned_model(base_model, CKPT_DIR)
    
    # åˆ›å»ºç”Ÿæˆé…ç½®
    generation_config = create_generation_config(
        temperature=TEMPERATURE,
        top_p=TOP_P,
        no_repeat_ngram_size=NO_REPEAT_NGRAM_SIZE
    )
    generation_config.pad_token_id = tokenizer.pad_token_id
    
    # è¯»å–æµ‹è¯•æ•°æ®
    try:
        with open(TEST_DATA_PATH, "r", encoding="utf-8") as f:
            test_datas = json.load(f)
        print(f"ğŸ“– è¯»å– {len(test_datas)} ä¸ªæµ‹è¯•æ ·ä¾‹")
    except FileNotFoundError:
        print(f"âŒ æµ‹è¯•æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {TEST_DATA_PATH}")
        print("ğŸ“ ä½¿ç”¨é»˜è®¤æµ‹è¯•æ•°æ®")
        test_datas = [
            {"instruction": "è¯·å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„å”è¯—", "input": ""},
            {"instruction": "è¯·å†™ä¸€é¦–å…³äºæœˆäº®çš„å”è¯—", "input": ""},
            {"instruction": "è¯·å†™ä¸€é¦–å…³äºå±±æ°´çš„å”è¯—", "input": ""}
        ]
    
    # æ‰¹é‡ç”Ÿæˆç»“æœ
    print("\nğŸ¯ å¼€å§‹æ‰¹é‡ç”Ÿæˆ...")
    results = []
    
    with open(output_path, "w", encoding="utf-8") as f:
        f.write(f"æ¨¡å‹æµ‹è¯•ç»“æœ\n")
        f.write(f"æ¨¡å‹: {CURRENT_MODEL}\n")
        f.write(f"æ£€æŸ¥ç‚¹: {CKPT_DIR}\n")
        f.write(f"æµ‹è¯•æ—¶é—´: {torch.cuda.get_device_name() if torch.cuda.is_available() else 'CPU/MPS'}\n")
        f.write("=" * 50 + "\n\n")
        
        for i, data in enumerate(test_datas):
            print(f"\nâ¡ï¸ ç¬¬ {i+1}/{len(test_datas)} ä¸ªæ ·ä¾‹")
            print(f"æŒ‡ä»¤: {data['instruction']}")
            if data.get('input'):
                print(f"è¾“å…¥: {data['input']}")
            
            try:
                output = evaluate(
                    instruction=data["instruction"],
                    tokenizer=tokenizer,
                    model=model,
                    generation_config=generation_config,
                    max_len=MAX_LEN,
                    input_text=data.get("input", ""),
                    verbose=False
                )
                
                # ä¿å­˜ç»“æœ
                result_entry = {
                    "instruction": data["instruction"],
                    "input": data.get("input", ""),
                    "output": output,
                    "status": "success"
                }
                results.append(result_entry)
                
                # å†™å…¥æ–‡ä»¶
                f.write(f"æ ·ä¾‹ {i+1}:\n")
                f.write(f"æŒ‡ä»¤: {data['instruction']}\n")
                if data.get('input'):
                    f.write(f"è¾“å…¥: {data['input']}\n")
                f.write(f"ç”Ÿæˆ: {output}\n")
                f.write("-" * 30 + "\n\n")
                
                print(f"âœ… ç”ŸæˆæˆåŠŸ: {output}")
                
            except Exception as e:
                error_msg = f"ç”Ÿæˆå¤±è´¥: {str(e)}"
                print(f"âŒ {error_msg}")
                
                result_entry = {
                    "instruction": data["instruction"],
                    "input": data.get("input", ""),
                    "output": "",
                    "status": "error",
                    "error": error_msg
                }
                results.append(result_entry)
                
                # å†™å…¥é”™è¯¯ä¿¡æ¯
                f.write(f"æ ·ä¾‹ {i+1}:\n")
                f.write(f"æŒ‡ä»¤: {data['instruction']}\n")
                if data.get('input'):
                    f.write(f"è¾“å…¥: {data['input']}\n")
                f.write(f"é”™è¯¯: {error_msg}\n")
                f.write("-" * 30 + "\n\n")
    
    # ç»Ÿè®¡ç»“æœ
    successful_count = sum(1 for r in results if r["status"] == "success")
    total_count = len(results)
    
    print(f"\nğŸ‰ æµ‹è¯•å®Œæˆï¼")
    print(f"ğŸ“Š æˆåŠŸç‡: {successful_count}/{total_count} ({successful_count/total_count*100:.1f}%)")
    print(f"ğŸ’¾ è¯¦ç»†ç»“æœä¿å­˜åœ¨: {output_path}")
    
    # æ˜¾ç¤ºç¤ºä¾‹ç»“æœ
    print_results_sample(results, num_samples=3)
    
    return results

def main():
    """
    ä¸»å‡½æ•°
    """
    test()

if __name__ == "__main__":
    main()