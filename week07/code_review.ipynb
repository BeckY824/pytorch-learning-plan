{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2630e01c",
   "metadata": {},
   "source": [
    "# 整理微调模型的代码(LLM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbbf6a7",
   "metadata": {},
   "source": [
    "## 配置文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6db9ced5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "MacBook Air M4 LoRA 训练脚本 - 使用用户指定的参数配置\n",
    "解决MPS设备的兼容性问题\n",
    "\"\"\"\n",
    "import os\n",
    "# 在导入 Transformers 之前设置环境变量\n",
    "os.environ[\"ACCELERATE_MIXED_PRECISION\"] = \"no\"\n",
    "os.environ[\"ACCELERATE_USE_FP16\"] = \"false\" \n",
    "os.environ[\"ACCELERATE_USE_BF16\"] = \"false\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "\n",
    "# 导入需要使用到的库\n",
    "import sys\n",
    "import json\n",
    "import warnings\n",
    "import logging\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import bitsandbytes as bnb\n",
    "from datasets import load_dataset, load_from_disk\n",
    "import transformers\n",
    "from peft import PeftModel\n",
    "from colorama import Fore, Style\n",
    "\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    GenerationConfig\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    prepare_model_for_kbit_training\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e8e9a0",
   "metadata": {},
   "source": [
    "## 使用到的方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "399409a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_data(data_point):\n",
    "    \"\"\"\n",
    "    将输入和输出文本转换为模型可读的 tokens。\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 构建完整的输入提示词\n",
    "        prompt = f\"\"\"[INST] <<SYS>>\n",
    "You are a helpful assistant and good at writing Tang poem. 你是一個樂於助人的助手且擅長寫唐詩。\n",
    "<</SYS>>\n",
    "\n",
    "{data_point[\"instruction\"]}\n",
    "{data_point[\"input\"]}\n",
    "[/INST]\"\"\"\n",
    "\n",
    "        # 计算用户提示词的 token 数量\n",
    "        prompt_tokenized = tokenizer(\n",
    "            prompt,\n",
    "            truncation=True, #如果文本太长，超过模型所支持的最大长度，自动截断\n",
    "            max_length=CUTOFF_LEN,\n",
    "            padding=False,\n",
    "            return_tensors=None\n",
    "        )\n",
    "        len_user_prompt_tokens = len(prompt_tokenized['input_ids'])\n",
    "\n",
    "        # 将完整的输入和输出转换为 tokens\n",
    "        full_text = prompt + \" \" + data_point[\"output\"] + \"</s>\"\n",
    "        full_tokenized = tokenizer(\n",
    "            full_text,\n",
    "            truncation=True,\n",
    "            max_length=CUTOFF_LEN,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=None\n",
    "        )\n",
    "\n",
    "        input_ids = full_tokenized['input_ids']\n",
    "        attention_mask = full_tokenized[\"attention_mask\"]\n",
    "\n",
    "        # 创建labels，屏蔽提示词部分\n",
    "        labels = input_ids.copy()\n",
    "        for i in range(min(len_user_prompt_tokens, len(labels))):\n",
    "            labels[i] = -100\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"labels\": labels,\n",
    "            \"attention_mask\": attention_mask,\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"数据处理错误: {e}\")\n",
    "        # 返回默认的数据\n",
    "        return {\n",
    "            \"input_ids\": [0] * CUTOFF_LEN,\n",
    "            \"labels\": [-100] * CUTOFF_LEN,\n",
    "            \"attention_mask\": [1] * CUTOFF_LEN,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc0ce65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(instruction, generation_config, max_len, input_text=\"\", verbose=True):\n",
    "    \"\"\"\n",
    "    使用 QWEN 格式生成响应\n",
    "    \"\"\"\n",
    "    prompt = (\n",
    "        \"<|im_start|>system\\n你是一位擅长写唐诗的中文助手。\\n<|im_end|>\\n\"\n",
    "        f\"<|im_start|>user\\n{instruction}\\n{input_text}\\n<|im_end|>\\n\"\n",
    "        \"<|im_start|>assistant\\n\"\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "    input_ids = input[\"input_ids\"]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        generation_output = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            generation_config=generation_config,\n",
    "            max_new_tokens=max_len,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True\n",
    "        )\n",
    "\n",
    "    output = tokenizer.decode(generation_output.sequences[0], skip_special_tokens=False)\n",
    "    # 清洗输出：阶段 assistant 开头后面的内容\n",
    "    if \"<|im_start|>assistant\" in output:\n",
    "        output = output.split(\"<|im_start|>assistant\")[1]\n",
    "    if \"<|im_end|>\" in output:\n",
    "        output = output.split(\"<|im_end|>\")[0]\n",
    "    output = output.strip()\n",
    "\n",
    "    if verbose:\n",
    "        print(output)\n",
    "    return output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "image",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
