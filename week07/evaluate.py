def evaluate(instruction, generation_config, max_len, input_text="", verbose=True):
    """
    ä½¿ç”¨ QWEN æ ¼å¼ç”Ÿæˆå“åº”
    """
    prompt = (
        "<|im_start|>system\nä½ æ˜¯ä¸€ä½æ“…é•¿å†™å”è¯—çš„ä¸­æ–‡åŠ©æ‰‹ã€‚\n<|im_end|>\n"
        f"<|im_start|>user\n{instruction}\n{input_text}\n<|im_end|>\n"
        "<|im_start|>assistant\n"
    )

    inputs = tokenizer(prompt, return_tensors='pt').to(device)
    input_ids = input["input_ids"]

    with torch.no_grad():
        generation_output = model.generate(
            input_ids=input_ids,
            generation_config=generation_config,
            max_new_tokens=max_len,
            return_dict_in_generate=True,
            output_scores=True
        )

    output = tokenizer.decode(generation_output.sequences[0], skip_special_tokens=False)
    # æ¸…æ´—è¾“å‡ºï¼šé˜¶æ®µ assistant å¼€å¤´åé¢çš„å†…å®¹
    if "<|im_start|>assistant" in output:
        output = output.split("<|im_start|>assistant")[1]
    if "<|im_end|>" in output:
        output = output.split("<|im_end|>")[0]
    output = output.strip()

    if verbose:
        print(output)
    return output

def print_results_sample(results, num_samples=3):
    print("\nğŸ“Œ ç¤ºä¾‹ç»“æœå±•ç¤º:")
    for i, result in enumerate(results[:num_samples]):
        print(f"\nç¬¬ {i+1} ä¸ªæ ·ä¾‹:")
        print(f"è¾“å…¥: {result['input']}")
        print(f"ç”Ÿæˆ: {result['output']}")
        print("-" * 50)